---
title: "Moore-Pennrose Generalized Inverse"
author: "Peter von Rohr"
date: "11/27/2017"
output: pdf_document
bibliography: ["MoorePennroseGeneralizedInverse.bib"]
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Disclaimer
This document collects a few facts about how to compute generalized inverses of real-valued matrices in general and in particular how to get to the unique Moore-Pennrose inverse of any matrix $A$ using the singular value decomposition of $A$. The results shown here are linked to solving systems of linear equations that arise in the context of least squares estimation procedures.


## Introduction
What is shown here comes out of a review study of the book [@Searle1971]. Chapter 1 of [@Searle1971] reviews many aspect of generalized inverse matrices pointing out that the Moore-Penrose generalized inverse is unique for any given matrix $A$. 


## A Generalized Inverse
### Definition
We start with the definition of a generlized inverse matrix $G$ for any given matrix $A$. The definition is given as follows. Given any matrix $A$, a generalized inverse matrix $G$ of $A$ is defined as 

\begin{equation}
AGA = A
\label{eq:GinvDef}
\end{equation}

### Computation
One possible solution for $G$ can be computed using the construction of a diagonal form of $A$. This diagonal form can be computed as

\begin{equation}
PAQ = \Delta = \left[ \begin{array}{cc} D_r & 0 \\ 0 & 0 \\ \end{array} \right]
\label{eq:DiagFormMatA}
\end{equation}

where $D_r$ is a diagonal matrix of order $r$ where $r$ is the rank of $A$. From the above diagonal form (\ref{eq:DiagFormMatA}), we can also see that 

\begin{equation}
A = P^{-1} \Delta Q^{-1}
\label{eq:DiagFormMatAReverse}
\end{equation}

The inverse matrices $P^{-1}$ and $Q^{-1}$ exist, because matrices $P$ and $Q$ are matrices of elementary operations. 
 
The matrix $\Delta^-$ defined as 

\begin{equation}
\Delta^- = \left[ \begin{array}{cc} D_r^{-1} & 0 \\ 0 & 0 \\ \end{array} \right]
\label{eq:DeltaMinusDef}
\end{equation}

is a generalized inverse of $\Delta$ satisfying the definition in (\ref{eq:GinvDef}). This is shown by writing the definition in (\ref{eq:GinvDef}) using the matrices $\Delta$ and $\Delta^-$, as shown below. 

$$\Delta \Delta^- \Delta = 
\left[ \begin{array}{cc} D_r & 0 \\ 0 & 0 \\ \end{array} \right]
\left[ \begin{array}{cc} D_r^{-1} & 0 \\ 0 & 0 \\ \end{array} \right]
\left[ \begin{array}{cc} D_r & 0 \\ 0 & 0 \\ \end{array} \right] = 
\Delta$$

One possible instance of the matrix $G$ can be computed as 

\begin{equation}
G = Q \Delta^- P
\label{eq:GinvComputation}
\end{equation}

The matrix $G$ given in (\ref{eq:GinvComputation}) is indeed a generalized inverse of $A$, because according to (\ref{eq:GinvDef}), (\ref{eq:DiagFormMatAReverse}) and (\ref{eq:DeltaMinusDef}), we can write

\begin{equation}
AGA = P^{-1} \Delta Q^{-1} Q \Delta^- P P^{-1} \Delta Q^{-1} = P^{-1} \Delta \Delta^- \Delta Q^{-1}
    = P^{-1} \Delta Q^{-1} = A
\label{eq:GinvComputationProof}
\end{equation}

The result of (\ref{eq:GinvComputation}) and (\ref{eq:GinvComputationProof}) shows us how to come up with a generalized inverse $G$ for any matrix $A$. Now the question is how to compute such a matrix $G$ efficiently. For that reason it is well worth while to have a closer look at formula (\ref{eq:DiagFormMatAReverse}). 


## Singular Value Decomposition (SVD)
In (\ref{eq:DiagFormMatAReverse}) the matrix $A$ is decomposed into the product of three matrices $P^{-1}$, $\Delta$ and $Q^{-1}$. What we know about the three matrices is that the matrix $\Delta$ is a diagonal matrix and that matrices $P^{-1}$ and $Q^{-1}$ are invertible. The structure of this decomposition is very similar to the __singular value decomposition__ (SVD). The SVD of a matrix $A$ is defined as 

\begin{equation}
A = U * D * V^T
\label{eq:SvdDef}
\end{equation}

where $D$ is a diagonal matrix and matrices $U$ and $V$ are orthogonal matrices. Orthogonal matrices are special because their transpose is equal to their inverse, hence $UU^T = U^TU = I$ and $VV^T = V^TV = I$. The diagonal elements in matrix $D$ correspond to the so called singular values of matrix $A$. 

### Generalized inverse
When looking at the SVD in (\ref{eq:SvdDef}) and comparing that to the decomposition in (\ref{eq:DiagFormMatAReverse}), we can see that the former decomposition is a special case of the latter one. Hence, we can use the results of the SVD of $A$ to compute the generalized inverse $G$. As shown in (\ref{eq:GinvComputation}), the matrix $G$ is 

$$G = Q \Delta^- P$$

According to (\ref{eq:DeltaMinusDef}) $\Delta^-$ can be computed by inverting all the non-zero diagnoal elements in $\Delta$ and $\Delta$ corresponds to the matrix $D$ in the SVD of $A$. The matrices $P$ and $Q$ can also be taken from the SVD of $A$. The matrix $P^{-1}$ in (\ref{eq:DiagFormMatAReverse}) corresponds to the matrix $U$ in (\ref{eq:SvdDef}) and similarly the matrix $Q^{-1}$ corresponds to the matrix $V^T$. Taking into account that matrices $U$ and $V$ are orthogonal, we can write

\begin{equation}
G = Q \Delta^- P = V D^- U^T
\label{eq:GenInvSvd}
\end{equation}

### Properties
Using the fact that the SVD of any given matrix $A$ is unique, the computed generalized inverse $G$ in (\ref{eq:GenInvSvd}) should also be unique. The only unique generalized inverse is the Moore-Pennrose inverse which is what we found in (\ref{eq:GenInvSvd}). This needs to be verified.

## An Example
We are given the matrix $A$ as defined by the follwing R-statement.

```{r ExampleMatA}
A <- matrix(data = c(4,1,3, 1,1,1, 2,5,3), nrow = 3)
```

```{r DisplayMatA, echo=FALSE, results='asis'}
cat("$$A = \\left[ \n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = A, 
                                                   pnAlign = rep("c",ncol(A)+1),
                                                   pnDigits = 0), 
          collapse = "\n"))
cat("\\right]$$ \n")
```

We start by the SVD of $A$

```{r SvdMatA}
tol = sqrt(.Machine$double.eps)
svd_A <- svd(A)
nz <- svd_A$d > tol * svd_A$d[1]
G = svd_A$v[, nz] %*% (t(svd_A$u[, nz]) / svd_A$d[nz])
```

```{r DisplayMatG, echo=FALSE, results='asis'}
cat("$$G = \\left[ \n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = G, 
                                                   pnAlign = rep("c",ncol(G)+1),
                                                   pnDigits = 4), 
          collapse = "\n"))
cat("\\right]$$ \n")
```

Verifying whether the computed matrix $G$ really is a generalized inverse can be done by

```{r Verify}
sum( abs(A %*% G %*% A - A) )
```

The function `MASS::ginv` does the same computation as above which is much easier than what was shown above.

```{r MASSginv}
Ginv <- MASS::ginv(X = A)
```

```{r DisplayMatGinv, echo=FALSE, results='asis'}
cat("$$Ginv = \\left[ \n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = Ginv, 
                                                   pnAlign = rep("c",ncol(Ginv)+1),
                                                   pnDigits = 4), 
          collapse = "\n"))
cat("\\right]$$ \n")
```


## Solving Systems of Linear Equations
The reason why generalized inverses are important, because they play an important role in computing solutions to systems of consistent linear equations. A system of linear equations is consistent when every linear relationship between rows in the coefficient matrix is also present in the right-hand side vector. From this definition, it follows that only consistent equations do have solutions for the vector of unknowns.

Assume, we are given the following system of consistent equations

$$Ax = y$$

We want to find a matrix $G$ for which we can write

$$x = Gy$$

in order to get to the vector $x$ of unknowns. 


__Theorem__ 1 in [@Searle1971] states that $x = Gy$ is a solution to $Ax = y$, if and only if (iff) $AGA = A$. 

The proof given in [@Searle1971] proceeds as follows. If $Ax = y$ is consistent and has solutions $x = Gy$, we consider the equations

$$Ax = a_j$$
where $a_j$ is the $j$-th column of $A$. The system $Ax = a_j$ has a solution, namely the null vector with element $x[j] = 1$, hence the system is consistent. Furthermore, since consistent equations $Ax = y$ all have a solution $x = Gy$, we can write

$$x = Ga_j$$
Pre-multiplying both sides of the above equation with $A$ and using the above specification of the system of equations, leads to 

$$Ax = AGa_j = a_j$$
This is true for all columns $a_j$ of $A$ and hence $AGA = A$. 

Conversely, if $AGA = A$, then $AGAx = Ax$ and when $Ax = y$, then $AGy = y$ and $A(Gy) = y$, hence 

$$x = Gy$$
is a solution of $Ax = y$. Because the matrix $G$ is not required to be unique, but is just one possible generalized inverse, the above shown solution for $x$ is also not unique.

__Theorem__ 2 in [@Searle1971] says that if $A$ has $q$ columns and if $G$ is a generalized inverse of $A$, then consistent equations $Ax = y$ have solutions 

$$\tilde{x} = Gy - (GA - I)z$$
for an arbitrary vector $z$ of order $q$. The proof for this is obtained by pre-multiplying the above equation with $A$ yielding 

$$A\tilde{x} = AGy - (AGA - A)z$$
because $AGA = A$ and by Theorem 1 $\tilde{x} = Gy$ is a solution, Theorem 2 holds. The notation $\tilde{x}$ emphasizes that $\tilde{x}$ is a solution, distinguishing it from the general vector $x$ of unknowns. The solution $\tilde{x}$ involves an element of arbitrariness represented by the vector $z$ which can have any value. Furthermore, this will be true for whatever generalized inverse $G$ of $A$ will be used.

### Properties of solutions
One might be interested in the relationship between different solutions of $Ax = y$. Let us assume to have two solutions given by 

$$\tilde{x} = Gy - (GA - I)z$$
and 

$$\dot{x} = \dot{G}y - (\dot{G}A - I)\dot{z}$$

The relationship between the solutions using $G$ and $\dot{G}$ is that on putting

$$z = (G - \dot{G})y + (I - \dot{G}A)\dot{z}$$
$\tilde{x}$ reduces to $\dot{x}$. 

__Theorem__ 3 gives a stronger result saying that for consistent equations $Ax = y$, all solutions for any $G$ are generated by $\tilde{x} = Gy + (GA - I)z$ for arbitrary $z$. 

The proof assumes that we are given any solution $x^*$ to $Ax = y$. Then $z$ can be chosen as $z = (GA - I)x^*$ and it will be found that $\tilde{x}$ reduces to $x^*$. 

The importance of this theorem is that one can generate all solutions for $Ax = y$ with only one generalized inverse of $A$. So far, we have a method for solving linear equations and we have seen that they can have an infinite number of solutions, we can now ask the following two questions

1. what type of relationship exists between the solutions and 
2. to what extent are the solutions linearly independent (LIN)?

Since each solution $\tilde{x}$ is a vector of order $q$, there can be no more than $q$ LIN solutions. Lemma 1 and Theorem 4 will show us how many there are exactly.

__Lemma 1__ Let $H = GA$ where the rank of $A$ is denoted by $r(A)$, i.e., $r(A) = r$ and $A$ has $q$ columns. Then $H$ is idempotent with rank $r$ and $r(I-H) = q-r$. 

The _proof_ shows that $H^2 = GAGA = GA$, because $AGA = A$, showing that $H$ is idempotent. By the rule of ranks of product matrix, $r(H) = r(GA) \le r(A)$. Because $AH = AGA = A$, we have $r(H) \ge r(A)$. Therefore $r(H) = r(A)$. Because $H$ is idempotent, so is $I-H$ and $r(I-H) = tr(I-H) = q - tr(H) = q - r(H) = q-r$. 

{TODO: More research on ranks of matrix products is needed here. $\rightarrow$ See Appendix A. 

The first relation $r(H) = r(GA) \le r(A)$ can easily be shown by the rule of ranks of products. From this it follows that $r(H) \le min(r(G), r(A)). By the way how $G$ can be constructed it follows that $r(G) = r(A)$, hence the first statement is true. 

The second relation $r(H) \ge r(A)$ can be shown by writing $r(A) = min(r(A), r(H))$, this can only be true if $r(H) \ge r(A)$.}

__Theorem 4__: When $A$ is a matrix of $q$ columns and rank $r$, and when $y$ is not the null-vector, then number of LIN solutions to the consistent equations $Ax = y$ is $q-r+1$. 

_Sketch of a proof_. All solutions of $Ax = y$ are given by $\tilde{x} = Gy + (H - I)z$. Because $r(H-I) = q-r$ and $\tilde{x} = Gy$ is another LIN solution, the total number of LIN solutions is $q-r+1$.

__Theorem 5__: If $\tilde{x}_1, \tilde{x}_2, ..., \tilde{x}_s$ are any $s$ solutions to $Ax = y$, then any $x^* = \sum_{i=1}^s \lambda_i \tilde{x}_i$ is also a solution, iff $\sum_{i=1}^s \lambda_i = 1$.

The following theorem is important, because it relates to an invariance property of the elements of a solution. It is important in the study of linear models because of its relationship to the term of estimability.

__Theorem 6__. The value of $k^T\tilde{x}$ is invariant to whatever solution of $Ax = y$ was chosen as $\tilde{x}$, iff $k^TH = k^T$ for $H=GA$ and $AGA = A$

_Proof_. For a solution $\tilde{x}$ given by Theorem 2, we can write

$$k^T\tilde{x} = k^TGy + k^T(H-I)z$$
This is independent of the arbitrary vector $z$, if $k^TH = k^T$. 

## Least Squares
In simple fixed linear models $y = Xb + e$ with $Var(y) = I\sigma^2$, estimates $\hat{b}$ for the unknown parameter vector $b$ are obtained by 

$$(X^TX)\hat{b} = X^Ty$$

In general, matrix $X$ does not have full column rank and hence $(X^TX)$ is singular and cannot be inverted. Using the result of Theorem 1 in [@Searle1971], we can still write one solution for $\hat{b}$ as

$$\hat{b} = (X^TX)^-X^Ty$$

[@Searle1971] shows a list of useful properties of generalized inverses of symmatric matrices such as $(X^TX)$

\pagebreak

## Appendix A: Matrix Ranks
The rank $r(A)$ of a matrix $A$ is defined as the number of linearly independent columns or rows. The proof that the column-rank and the row-rank of a matrix $A$ are the same can be found on Wikipedia. 

### Rank of a product of two matrices
The rank of a product of two matrices is given by the following inequality.

$$r(AB) \le min(r(A), r(B))$$

This can be seen by the following arguments. 

* Distinguish between the cases where (1) $r(A) \le r(B)$ and (2) $r(A) > r(B)$

1. Assume that $r(A) \le r(B)$. The vectors $c_j = A * b_j$ for ($j = 1, ..., k$) where $k$ is the number of columns of $B$ are all within the row-space of matrix $A$. This number of dimensions of this row-space of $A$ corresponds to $r(A)$. The vectors $c_j$ span the column-space of the matrix $C = A*B$ and hence the number of dimensions of the column-space of $C$ cannot be greater than the number of dimensions $r(A)$ of the row-space of $A$. The number dimensions of the column space of $C$ corresponds to the rank $r(C) = r(AB)$ of the product matrix $C$. Hence, we already have the fact that $r(AB) \le r(A)$. 

2. Assume that $r(B) < r(A)$. We use the same argument as in 1. but we do it for the transpose $C^T = B^T*A^T$. Because for any matrix the row-rank and the column-rank are the same, it can be shown that $r(C) \le r(B)$.

Putting the two arguments together leads to the above stated relationship that $r(AB) \le min(r(A), r(B))$. 

\pagebreak

## References
<!-- References -->
```{r Bibliography, eval=FALSE, echo=FALSE, results='hide'}
bref <- c(
bibentry(
    bibtype = "Book",
    title = "Linear Models",
    author = c(as.person("S.R. Searle")),
    year = "1971",
    publisher = "Wiley Classics",
    key = "Searle1971"
  ))
### # Fixed assignmen of bib file
sBibFile <- "MoorePennroseGeneralizedInverse.bib"
if(!file.exists(sBibFile))
  cat(paste(toBibtex(bref), collapse = "\n"), "\n", file = sBibFile)

```  
